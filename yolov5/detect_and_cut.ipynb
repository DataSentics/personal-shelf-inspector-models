{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, non_max_suppression, apply_classifier, scale_coords, xyxy2xywh, \\\n",
    "    plot_one_box, strip_optimizer, set_logging, increment_dir\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.amlignore', '.amlignore.amltmp', '.dockerignore', '.git', '.gitattributes', '.github', '.gitignore', '.ipynb_aml_checkpoints', '.ipynb_checkpoints', 'data', 'detect.py', 'detect.sh', 'detect_and_cut.ipynb', 'detect_and_cut.ipynb.amltmp', 'Dockerfile', 'evolve.txt', 'hubconf.py', 'LICENSE', 'model.ipynb', 'model.py', 'models', 'other_commands.txt', 'prepare_data.sh', 'README-ORIG.md', 'README.md', 'requirements.txt', 'runs', 'test.py', 'train.py', 'train_general.sh', 'train_price.sh', 'tutorial.ipynb', 'utils', 'via_to_coco.py', 'weights', 'yolov5s.pt', '__pycache__']\n",
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\".\"))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir, source, weights, view_img, save_txt, imgsz = \\\n",
    "#         Path(opt.save_dir), opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Namespace()\n",
    "opt.weights = \"../../../pricetag_reader/yolov5/weights/pricetag_small.pt\"\n",
    "opt.source = \"../../../pricetag_reader/kaufland_photos\"\n",
    "# opt.weights = \"pricetag_reader/yolov5/weights/pricetag_small.pt\"  # product name and price model #\"runs/train/exp8/weights/best.pt\"\n",
    "# opt.source = \"../kaufland_photos\"  #\"data/images\"\n",
    "opt.conf_thres = 0.25\n",
    "opt.iou_thres = 0.45\n",
    "opt.save_txt = False\n",
    "opt.view_img = False\n",
    "opt.augment = False\n",
    "opt.update = False\n",
    "opt.device = \"cpu\"\n",
    "opt.img_size = 640\n",
    "opt.classes = None\n",
    "opt.agnostic_nms = False\n",
    "opt.save_img = False\n",
    "opt.name = \"\"\n",
    "# save only cut detections\n",
    "opt.save_cut = True\n",
    "\n",
    "webcam = False\n",
    "save_dir = Path(\"../../../pricetag_reader/kaufland_photos_p\") #Path('runs/detect')\n",
    "# parser.add_argument('--source', type=str, default='data/images', help='source')  # file/folder, 0 for webcam\n",
    "# parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
    "# parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n",
    "# parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
    "# parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
    "# parser.add_argument('--view-img', action='store_true', help='display results')\n",
    "# parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
    "# parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
    "# parser.add_argument('--save-dir', type=str, default='runs/detect', help='directory to save results')\n",
    "# parser.add_argument('--name', default='', help='name to append to --save-dir: i.e. runs/{N} -> runs/{N}_{name}')\n",
    "# parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
    "# parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "# parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "# parser.add_argument('--update', action='store_true', help='update all models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "90 files found\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "device = select_device(opt.device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(opt.weights, map_location=opt.device)  # load FP32 model\n",
    "imgsz = check_img_size(opt.img_size, s=model.stride.max())  # check img_size\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "\n",
    "# Second-stage classifier\n",
    "classify = False\n",
    "if classify:\n",
    "    modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "    modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n",
    "    modelc.to(device).eval()\n",
    "\n",
    "# Set Dataloader\n",
    "#save_img = False # True\n",
    "dataset = LoadImages(opt.source, img_size=opt.img_size)\n",
    "print(\"{} files found\".format(len(dataset.files)))\n",
    "\n",
    "# Get names and colors\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/20200827_152613.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/20200827_152613.jpg\n",
      "(3, 640, 384)\n",
      "640x384 19 prices, Done. (0.075s)\n",
      "image 2/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171636.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171636.jpg\n",
      "(3, 480, 640)\n",
      "480x640 16 prices, Done. (0.091s)\n",
      "image 3/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171641.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171641.jpg\n",
      "(3, 480, 640)\n",
      "480x640 7 prices, Done. (0.092s)\n",
      "image 4/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171647.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20181212_171647.jpg\n",
      "(3, 480, 640)\n",
      "480x640 5 prices, Done. (0.097s)\n",
      "image 5/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20200923_104028.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20200923_104028.jpg\n",
      "(3, 640, 480)\n",
      "640x480 6 prices, Done. (0.092s)\n",
      "image 6/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20200923_104032.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20200923_104032.jpg\n",
      "(3, 640, 480)\n",
      "640x480 8 prices, Done. (0.105s)\n",
      "image 7/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185005.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185005.jpg\n",
      "(3, 640, 480)\n",
      "640x480 5 prices, Done. (0.090s)\n",
      "image 8/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185021.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185021.jpg\n",
      "(3, 640, 480)\n",
      "640x480 6 prices, Done. (0.091s)\n",
      "image 9/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185128.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185128.jpg\n",
      "(3, 640, 480)\n",
      "640x480 7 prices, Done. (0.095s)\n",
      "image 10/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185133.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185133.jpg\n",
      "(3, 640, 480)\n",
      "640x480 5 prices, Done. (0.093s)\n",
      "image 11/90 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185142.jpg: /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/pricetag_reader/kaufland_photos/IMG_20210222_185142.jpg\n",
      "(3, 640, 480)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-45a397b365c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_synchronized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Apply NMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/models/yolo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, profile)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# augmented inference, train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# single-scale inference, train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/models/yolo.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, profile)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%10.1f%10.0f%10.1fms %-40s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/models/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/models/common.py\u001b[0m in \u001b[0;36mfuseforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfuseforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "if save_dir == Path('runs/detect'):  # if default\n",
    "    os.makedirs('runs/detect', exist_ok=True)  # make base\n",
    "    save_dir = Path(increment_dir(save_dir / 'exp', opt.name))  # increment run\n",
    "os.makedirs(save_dir / 'labels' if opt.save_txt else save_dir, exist_ok=True)  # make new dir\n",
    "\n",
    "\n",
    "# Run inference\n",
    "t0 = time.time()\n",
    "img = torch.zeros((1, 3, opt.img_size, opt.img_size), device=device)  # init img\n",
    "_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "\n",
    "n_photos = 0\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    print(path)\n",
    "# if path.split(\"/\")[-1] != \"20200827_152613.jpg\":\n",
    "#     continue\n",
    "#    print(path,\"------\")\n",
    "    print(img.shape)\n",
    "#     print(img[0,300:320,300:320])\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # uncomment this to process only given amount of photos  \n",
    "#     if n_photos > 40:\n",
    "#          break\n",
    "    n_photos += 1\n",
    "    \n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "    t2 = time_synchronized()\n",
    "    \n",
    "    # Apply Classifier\n",
    "    if classify:\n",
    "        pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        cnt = 0\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, s, im0 = Path(path[i]), '%g: ' % i, im0s[i].copy()\n",
    "        else:\n",
    "            p, s, im0 = Path(path), '', im0s\n",
    "\n",
    "        save_path = str(save_dir / p.name)\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # detections per class\n",
    "                s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if opt.save_txt:  # Write to file\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                    with open(txt_path + '.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line) + '\\n') % line)\n",
    "\n",
    "                if opt.save_img or opt.view_img:  # Add bbox to image\n",
    "                    label = '%s %.2f' % (names[int(cls)], conf)\n",
    "                    plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "                \n",
    "                if opt.save_cut: # Cut and save detections\n",
    "                    cut = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2]),:]\n",
    "                    cv2.imwrite(\"{}_{}.jpg\".format(save_path.split(\".jpg\")[0], cnt), cut)\n",
    "                cnt += 1\n",
    "                    \n",
    "                    \n",
    "        # Print time (inference + NMS)\n",
    "        print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
    "        break\n",
    "        \n",
    "        # Stream results\n",
    "        if opt.view_img:\n",
    "            cv2.imshow(p, im0)\n",
    "            if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "                raise StopIteration\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if opt.save_img:\n",
    "            if dataset.mode == 'images':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:\n",
    "                if vid_path != save_path:  # new video\n",
    "                    vid_path = save_path\n",
    "                    if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                        vid_writer.release()  # release previous video writer\n",
    "\n",
    "                    fourcc = 'mp4v'  # output video codec\n",
    "                    fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                    w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                    h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
    "                vid_writer.write(im0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/detect/exp11/IMG_20200901_173032.jpg'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "#int(xyxy[0])\n",
    "cut = im0[int(xyxy[1]):int(xyxy[3]),int(xyxy[0]):int(xyxy[2]),:]\n",
    "Image.fromarray(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[232, 232, 232],\n",
       "        [233, 233, 233],\n",
       "        [233, 233, 233],\n",
       "        ...,\n",
       "        [231, 229, 229],\n",
       "        [232, 230, 230],\n",
       "        [234, 232, 232]],\n",
       "\n",
       "       [[232, 232, 232],\n",
       "        [232, 232, 232],\n",
       "        [234, 234, 234],\n",
       "        ...,\n",
       "        [232, 230, 230],\n",
       "        [233, 231, 231],\n",
       "        [234, 232, 232]],\n",
       "\n",
       "       [[231, 231, 231],\n",
       "        [230, 230, 230],\n",
       "        [233, 233, 233],\n",
       "        ...,\n",
       "        [234, 232, 232],\n",
       "        [235, 233, 233],\n",
       "        [235, 233, 233]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[195, 193, 192],\n",
       "        [195, 193, 192],\n",
       "        [197, 195, 194],\n",
       "        ...,\n",
       "        [238, 236, 236],\n",
       "        [237, 235, 235],\n",
       "        [236, 234, 234]],\n",
       "\n",
       "       [[200, 198, 197],\n",
       "        [200, 198, 197],\n",
       "        [204, 202, 201],\n",
       "        ...,\n",
       "        [232, 230, 230],\n",
       "        [232, 230, 230],\n",
       "        [233, 231, 231]],\n",
       "\n",
       "       [[219, 217, 216],\n",
       "        [219, 217, 216],\n",
       "        [220, 218, 217],\n",
       "        ...,\n",
       "        [217, 215, 214],\n",
       "        [221, 219, 218],\n",
       "        [223, 221, 220]]], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"{}_{}.jpg\".format(save_path.split(\".jpg\")[0], cnt)\n",
    "#cv2.imwrite(cut, \"{}_{}.jpg\".format(save_path.split(\".jpg\")[0], cnt))\n",
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/data/images/bus.jpg: 640x480 Done. (0.086s)\n",
      "image 2/2 /mnt/batch/tasks/shared/LS_root/mounts/clusters/nc6-pm/code/Users/petr.michal/yolov5/data/images/zidane.jpg: 384x640 Done. (0.178s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Initialize\n",
    "# set_logging()\n",
    "# device = select_device(opt.device)\n",
    "# half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# # Load model\n",
    "# model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "# imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "# if half:\n",
    "#     model.half()  # to FP16\n",
    "\n",
    "# # Second-stage classifier\n",
    "# classify = False\n",
    "# if classify:\n",
    "#     modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "#     modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n",
    "#     modelc.to(device).eval()\n",
    "\n",
    "# # Set Dataloader\n",
    "# vid_path, vid_writer = None, None\n",
    "# if webcam:\n",
    "#     view_img = True\n",
    "#     cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "#     dataset = LoadStreams(source, img_size=imgsz)\n",
    "# else:\n",
    "#     save_img = True\n",
    "#     dataset = LoadImages(source, img_size=imgsz)\n",
    "\n",
    "# # Get names and colors\n",
    "# names = model.module.names if hasattr(model, 'module') else model.names\n",
    "# colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
    "\n",
    "# Run inference\n",
    "t0 = time.time()\n",
    "img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
    "_ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    pred = model(img, augment=opt.augment)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # Apply Classifier\n",
    "    if classify:\n",
    "        pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, s, im0 = Path(path[i]), '%g: ' % i, im0s[i].copy()\n",
    "        else:\n",
    "            p, s, im0 = Path(path), '', im0s\n",
    "\n",
    "        save_path = str(save_dir / p.name)\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n",
    "        s += '%gx%g ' % img.shape[2:]  # print string\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # detections per class\n",
    "                s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # Write to file\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                    with open(txt_path + '.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line) + '\\n') % line)\n",
    "\n",
    "                if save_img or view_img:  # Add bbox to image\n",
    "                    label = '%s %.2f' % (names[int(cls)], conf)\n",
    "                    plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
    "\n",
    "        # Print time (inference + NMS)\n",
    "        print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
    "\n",
    "        # Stream results\n",
    "        if opt.view_img:\n",
    "            cv2.imshow(p, im0)\n",
    "            if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "                raise StopIteration\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'images':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "            else:\n",
    "                if vid_path != save_path:  # new video\n",
    "                    vid_path = save_path\n",
    "                    if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                        vid_writer.release()  # release previous video writer\n",
    "\n",
    "                    fourcc = 'mp4v'  # output video codec\n",
    "                    fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                    w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                    h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
    "                vid_writer.write(im0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pricetag_reader",
   "language": "python",
   "name": "pricetag_reader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
